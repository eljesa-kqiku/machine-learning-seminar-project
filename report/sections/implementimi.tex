\section{Algorithm implementation}
Next, we will implement the three chosen algorithms: MLP (Multilayer Perceptron), Autoencoder, and Naive Bayes. These algorithms have been selected due to their suitability for classification problems and their ability to handle structured and complex datasets. First, we will provide a brief introduction to each algorithm, highlighting their key characteristics. After that, we will proceed with the implementation and performance analysis for our dataset.

\subsection{Multilayer Perceptron (MLP)}

The \textbf{Multilayer Perceptron (MLP)} is a core model in the field of machine learning, used for both classification and regression tasks. MLP consists of three primary layers: the \textbf{input layer}, one or more \textbf{hidden layers}, and the \textbf{output layer}. Each layer contains multiple neurons, and neurons from one layer are fully connected to neurons in the next layer. This dense connectivity allows MLP to capture complex relationships in data, particularly non-linear ones, which are common in many real-world applications \cite{deeplearning1}.

In the context of our dataset, which includes health-related features such as \texttt{BMI}, \texttt{Age}, \texttt{Physical\_Health}, and \texttt{Sleep\_Time}, MLP is well-suited to identify hidden patterns and make predictions about whether an individual has diabetes or not. The network learns from the data through a process called \textit{backpropagation}, where errors from the output are propagated back through the network to adjust the weights of the connections between neurons. This process is combined with \textit{gradient descent}, an optimization technique that minimizes the prediction error by adjusting the weights during each iteration of training. Over time, the model learns the optimal weights, improving its ability to make accurate predictions \cite{nn1}.

The basic structure of an MLP is illustrated in the figure below. The input features, such as \texttt{BMI} and \texttt{Age}, are fed into the input layer. From there, they pass through one or more hidden layers, where neurons transform the data by applying learned weights and activation functions. The transformed data then flows to the output layer, which produces the final prediction, such as whether an individual has diabetes.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{images/mlp-diagram.png}
    \caption{Basic structure of a Multilayer Perceptron (MLP) \cite{mlp}.}
    \label{fig:mlp}
\end{figure}

In the image, you can observe the flow of information from the input layer through the hidden layers to the output layer. Each layer plays a critical role in transforming the input data to make sense of the patterns and relationships. The neurons in the hidden layers learn features in the data, and as the network adjusts its weights through training, it becomes more accurate in predicting the target outcome.

MLPs are particularly effective in tasks where the relationships between input features and the target variable are non-linear, as is the case in health-related predictions like diabetes classification \cite{deeplearning1}. This model's ability to learn from complex and large datasets allows it to handle a wide variety of problems, from image recognition to medical diagnoses.

What makes MLPs particularly well-suited for our dataset is their capacity to model intricate relationships between multiple features simultaneously. By learning these relationships, MLPs can make accurate predictions, even when the data contains complex, interdependent factors. This makes MLP an ideal choice for predicting the likelihood of diabetes based on a variety of health indicators \cite{mlp}.

\subsubsection{Implementation and Evaluation}

The MLP model was implemented using the \texttt{scikit-learn} library, leveraging two hidden layers with sizes 100 and 50 respectively. The activation function used was ReLU, and stochastic gradient descent was chosen as the optimizer with adaptive learning rate and a maximum of 500 iterations.

Before finalizing the model, we performed hyperparameter tuning using GridSearchCV to identify the best combination of parameters. This ensures that the model generalizes well to unseen data.

\noindent
The best parameters found were:
\begin{quote}
\texttt{\{'mlp\_\_activation': 'relu', 'mlp\_\_alpha': 0.001, 'mlp\_\_hidden\_layer\_sizes': (100, 50), 'mlp\_\_learning\_rate': 'adaptive', 'mlp\_\_max\_iter': 500, 'mlp\_\_solver': 'sgd'\}}
\end{quote}

\begin{lstlisting}[language=Python, label={lst:mlp_classifier}]
    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    
    from sklearn.model_selection import train_test_split, cross_validate
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import (
        accuracy_score, precision_score, recall_score,
        f1_score, confusion_matrix, ConfusionMatrixDisplay,
        classification_report
    )
    
    # 1. Load data
    df = pd.read_csv("datasets/diabetes_scaled.csv")
    
    # 2. Features & Labels
    X = df.drop("Diabetes_binary", axis=1)
    y = df["Diabetes_binary"]
    
    # 3. Train/Test
    X_train = pd.read_csv("datasets/X_train.csv")
    X_test = pd.read_csv("datasets/X_test.csv")
    y_train = pd.read_csv("datasets/y_train.csv")
    y_test = pd.read_csv("datasets/y_test.csv")
    
    # 4. Define MLP model
    mlp = MLPClassifier(
        hidden_layer_sizes=(100, 50),
        activation='relu',
        solver='sgd',
        alpha=0.001,
        learning_rate='adaptive',
        max_iter=500,
        random_state=42
    )
    
    # 5. Train model
    mlp.fit(X_train, y_train)
    
    # 6. Predict on test set
    y_pred = mlp.predict(X_test)
    
    # Plot loss curve for full feature set
    plt.figure(figsize=(8, 4))
    plt.plot(mlp.loss_curve_)
    plt.title("Loss Curve (Full Feature Set)")
    plt.xlabel("Iterations")
    plt.ylabel("Loss")
    plt.grid(True)
    plt.show()
    
    # 7. Evaluate metrics
    print("Performance on Full Feature Set:")
    print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
    print(f"Precision: {precision_score(y_test, y_pred):.4f}")
    print(f"Recall: {recall_score(y_test, y_pred):.4f}")
    print(f"F1 Score: {f1_score(y_test, y_pred):.4f}")
    
    # 8. Confusion matrix visualization
    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=mlp.classes_)
    disp.plot(cmap=plt.cm.Blues)
    plt.title("Confusion Matrix (Full Feature Set)")
    plt.show()
    
    # 9. Cross-validation without plot
    cv_results = cross_validate(
        mlp, X_train, y_train,
        cv=5,
        scoring=["accuracy", "precision", "recall", "f1"],
        return_train_score=True
    )
    
    print("\nCross-Validation Mean Scores (5-Fold):")
    for metric in ["accuracy", "precision", "recall", "f1"]:
        test_score = np.mean(cv_results[f'test_{metric}'])
        train_score = np.mean(cv_results[f'train_{metric}'])
        print(f"{metric.capitalize()}: Train = {train_score:.4f}, Test = {test_score:.4f}")
    
    print("\nDetailed Classification Report (Full Feature Set):")
    print(classification_report(y_test, y_pred, target_names=["No Diabetes", "Diabetes"]))
    \end{lstlisting}
    
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{images/our-mlp-diagram.png} % Make sure the filename matches
    \caption{Our structure of a Multilayer Perceptron (MLP).}
    \label{fig:mlp-arch}
\end{figure}

\vspace{0.5em}

\noindent
Figure~\ref{fig:mlp-arch} illustrates the architecture of the Multilayer Perceptron used in our implementation. It includes an input layer that receives features such as \texttt{BMI}, \texttt{Age}, and \texttt{GenHealth}, followed by two hidden layers with 100 and 50 neurons respectively. These layers apply ReLU activation and are fully connected. The final layer outputs a binary prediction indicating whether the individual is diabetic or not.


\subsubsection{Results on Full Feature Set}

\begin{verbatim}
Performance on Full Feature Set:
Accuracy: 0.7526
Precision: 0.7362
Recall: 0.8011
F1 Score: 0.7673
\end{verbatim}

\noindent
This baseline performance shows that the MLP model handles the full feature set well, achieving over 75\% accuracy and strong recall (80.11\%). Precision and F1-score are also relatively balanced, which means the model is both identifying and distinguishing cases effectively.

\vspace{0.5em}
\noindent
Next, we look at cross-validation results to assess the model’s ability to generalize across different data splits:

\begin{verbatim}
Cross-Validation Mean Scores (5-Fold):
Accuracy: Train = 0.7558, Test = 0.7484
Precision: Train = 0.7359, Test = 0.7295
Recall: Train = 0.8113, Test = 0.8040
F1: Train = 0.7718, Test = 0.7649
\end{verbatim}

\noindent
These results confirm that the model generalizes well, as train and test scores remain consistent across folds. Importantly, test recall remains high (80.40\%), reinforcing the model’s reliability in correctly identifying diabetic cases in unseen data. This cross-validation step validates the single-run results and supports the model’s robustness.

\vspace{0.5em}
\noindent
Now we present the full classification report to explore class-wise performance in more detail:

\begin{verbatim}
Detailed Classification Report (Full Feature Set):
              precision    recall  f1-score   support

 No Diabetes       0.77      0.70      0.74      6753
    Diabetes       0.74      0.80      0.77      7000

    accuracy                           0.75     13753
   macro avg       0.75      0.75      0.75     13753
weighted avg       0.75      0.75      0.75     13753
\end{verbatim}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{images/confusion_matrix_full.png}
    \caption{Confusion Matrix for MLP on Full Feature Set}
\end{figure}

\noindent
From this report, it's evident that the model performs slightly better at detecting positive (diabetic) cases than negative ones, with a higher recall for the "Diabetes" class (80\%). This is desirable in our context: it is far more critical to minimize false negatives (undiagnosed diabetic patients) than false positives. Therefore, recall is the metric we prioritize most.

\subsubsection{Results on Selected Features with Interaction Terms}

\begin{verbatim}
Performance on Selected Features + Interactions:
Accuracy: 0.6917
Precision: 0.6633
Recall: 0.8007
F1 Score: 0.7256
\end{verbatim}

\noindent
While accuracy and precision have dropped in this simplified model, recall has remained nearly identical (80.07\%) to the full-feature model. This suggests that the reduced feature set is still effective in identifying diabetic patients, even if it leads to more false positives (lower precision).

\vspace{0.5em}
\noindent
We now show the detailed classification report for this model:

\begin{verbatim}
Detailed Classification Report (Interactions Model):
              precision    recall  f1-score   support

 No Diabetes       0.74      0.58      0.65      6753
    Diabetes       0.66      0.80      0.73      7000

    accuracy                           0.69     13753
   macro avg       0.70      0.69      0.69     13753
weighted avg       0.70      0.69      0.69     13753
\end{verbatim}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{images/confusion_matrix_interactions.png}
    \caption{Confusion Matrix for MLP on Selected Features + Interaction Terms}
\end{figure}

\noindent
Here again, recall for diabetic cases remains at 80\%, but performance for non-diabetic predictions drops noticeably (recall of 58\%). In practice, this means more people without diabetes are misclassified as diabetic. While this may raise false alarms, it is an acceptable trade-off in medical diagnostics where missing true cases is more dangerous than flagging potential ones.

\vspace{1em}
\noindent
In both models, recall is prioritized above all, aligning with our goal of reducing undiagnosed diabetic cases. The full-feature model is preferred for balanced performance, but the interaction-based model still holds value in more constrained environments.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{images/mlp-loss-curve.png} 
    \caption{Training loss curve for the MLP model over 500 iterations.}
    \label{fig:mlp_loss_curve}
    \end{figure}

The loss curve for the diabetes dataset shows a rapid initial decrease followed by convergence, indicating stable training and successful optimization.


\subsection{Autoencoder}
\input{sections/autoencoder}

\subsection{Naive Bayes}
The Naive Bayes algorithm offers a straightforward yet often surprisingly effective approach to supervised learning, particularly for classification tasks like predicting diabetes status. At its core, it leverages Bayes' Theorem, a fundamental concept in probability that allows us to update the probability of an event based on new evidence. What makes Naive Bayes "naive" is its key simplifying assumption: it treats every feature in the dataset as if it were completely independent of all other features, given the class label (in our case, whether an individual has diabetes or not).
The algorithm works by learning the conditional probability of each feature value for each class. For instance, it calculates the probability of having a certain BMI range given that the individual has diabetes, and the probability of having the same BMI range given that they do not have diabetes. It does this for all features in our dataset. When presented with a new individual's health profile, Naive Bayes uses these learned probabilities, along with the overall prevalence of diabetes in the training data (the prior probability), to calculate the probability of that individual belonging to each class (diabetes or no diabetes). The class with the highest calculated probability is then assigned as the prediction.
The mathematical foundation lies in Bayes' Theorem:

$$P(\text{Class}|\text{Features}) = \frac{P(\text{Class}) \times P(\text{Features}|\text{Class})}{P(\text{Features})}$$

The "naive" independence assumption simplifies the term $P(\text{Features}|\text{Class})$ into the product of the individual feature probabilities:

$$P(x_1, x_2, ..., x_n|\text{Class}) = P(x_1|\text{Class}) \times P(x_2|\text{Class}) \times ... \times P(x_n|\text{Class})$$


For our diabetes dataset, Naive Bayes presents a compelling choice due to its speed, ease of implementation, and its ability to effectively handle datasets with numerous features. Although the health indicators in the dataset are unlikely to be completely independent—a key assumption of Naive Bayes—the algorithm has demonstrated strong empirical success across various domains. This suggests that it can still provide valuable insights and deliver reliable predictive performance in identifying individuals at risk of diabetes.

There are several variants of Naive Bayes classifiers, each suited to different types of data. Gaussian Naive Bayes is commonly used when features are continuous and assumed to follow a normal distribution. Multinomial Naive Bayes typically applies to discrete features, while Bernoulli Naive Bayes is designed for binary or boolean features. Given that our dataset contains both continuous variables such as BMI and Age, as well as binary indicators like HighBP and Smoker status, Gaussian Naive Bayes is the most appropriate choice. This model assumes that the continuous features are normally distributed within each class, which aligns well with the nature of our data.

Overall, the simplicity of Naive Bayes makes it an excellent baseline model against which to compare more complex algorithms like Multilayer Perceptron (MLP) and Autoencoder. While it may not capture intricate interactions between features as effectively as MLP, Naive Bayes remains a solid and interpretable choice for predicting diabetes risk, offering a good balance between performance and computational efficiency.
\newline
\subsubsection{Implementation and Evaluation}

For the implementation of the Naive Bayes classifier, we utilized the Gaussian Naive Bayes variant due to the presence of both continuous and binary features in the dataset. The model was trained on the preprocessed diabetes dataset, where continuous features such as BMI and Age were assumed to follow a Gaussian distribution within each class. Binary features, including indicators like HighBP and Smoker, were also incorporated directly without additional transformation.

To ensure consistency in evaluation, we trained the model using the same training and testing sets applied in all algorithms we used. Additionally, we used 5-fold cross-validation to assess the model’s generalization ability.

To evaluate the performance of the Naive Bayes classifier, several metrics were employed, including accuracy, precision, recall, and the F1-score. Given the medical context and the importance of correctly identifying positive cases, special emphasis was placed on recall and F1-score to assess the model's ability to detect individuals with diabetes effectively.


Below is the Python code used for training and evaluating the Gaussian Naive Bayes classifier:

\lstinputlisting[language=python]{../code/naiveBayes.py}

\subsubsection{Results (Naive Bayes)}

\begin{verbatim}
Accuracy: 0.7381
Precision: 0.7300
Recall: 0.7558
F1 Score: 0.7427
\end{verbatim}
This initial evaluation of the Gaussian Naive Bayes model on the full feature set yields an accuracy of 0.7381. The model demonstrates a precision of 0.7300, indicating that when it predicts "Diabetes," it is correct approximately 73\% of the time. The recall of 0.7558 suggests that the model successfully identifies about 75.6\% of the actual diabetic cases. The F1-score, which balances precision and recall, is 0.7427.

\paragraph*{Detailed Classification Report (Naive Bayes):}
\begin{verbatim}
              precision    recall  f1-score   support

  No Diabetes       0.76      0.66      0.71      6753
     Diabetes       0.71      0.81      0.76      7000

     accuracy                           0.73     13753
    macro avg       0.74      0.74      0.73     13753
 weighted avg       0.73      0.73      0.73     13753
\end{verbatim}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{images/Figure_1.png}
    \caption{Confusion Matrix for Naive Bayes}
\end{figure}
The classification report offers a more granular view of the model's performance for each class. For the "No Diabetes" class, the precision is 0.76 and the recall is 0.66. This means that 76\% of the predictions for "No Diabetes" were correct, and 66\% of all actual "No Diabetes" cases were identified.

For the "Diabetes" class, the precision is 0.71, and the recall is 0.81. This indicates that 71\% of the "Diabetes" predictions were correct, and 81\% of all actual "Diabetes" cases were identified.

The overall accuracy is 0.73, consistent with the initial performance evaluation. The macro and weighted averages for precision, recall, and F1-score provide a consolidated view of the model's effectiveness across both classes. The higher recall for the "Diabetes" class suggests that the model is better at identifying positive cases, which is often a crucial consideration in medical applications.